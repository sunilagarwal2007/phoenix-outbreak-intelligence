#!/usr/bin/env python3
"""
Phoenix Outbreak Intelligence - Integration Test Suite
Comprehensive testing of multi-agent workflows and system integration
"""

import json
import asyncio
import logging
import sys
import os
import time
from pathlib import Path
from typing import Dict, List, Any, Optional
from unittest.mock import Mock, patch

# Add project root to path for imports
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class IntegrationTestSuite:
    """Comprehensive integration test suite for Phoenix system"""
    
    def __init__(self):
        self.test_results = []
        self.mock_data_loaded = False
        self.agents_initialized = False
        
    async def setup_test_environment(self):
        """Initialize test environment with mock data"""
        logger.info("ğŸ”§ Setting up test environment...")
        
        # Load test data
        try:
            with open('tests/demo_chat_cases.json', 'r') as f:
                self.test_cases = json.load(f)
            logger.info(f"âœ“ Loaded {len(self.test_cases)} test cases")
            self.mock_data_loaded = True
        except Exception as e:
            logger.error(f"âŒ Failed to load test cases: {e}")
            return False
            
        # Initialize mock agents
        try:
            await self._initialize_mock_agents()
            self.agents_initialized = True
            logger.info("âœ“ Mock agents initialized")
        except Exception as e:
            logger.error(f"âŒ Failed to initialize agents: {e}")
            return False
            
        return True
        
    async def _initialize_mock_agents(self):
        """Initialize mock versions of all agents for testing"""
        # Mock Data Intelligence Agent
        self.mock_data_agent = Mock()
        self.mock_data_agent.analyze_outbreak_risk = self._mock_risk_analysis
        
        # Mock Public Guidance Agent
        self.mock_guidance_agent = Mock()
        self.mock_guidance_agent.generate_public_guidance = self._mock_guidance_generation
        
        # Mock Resource Report Agent
        self.mock_resource_agent = Mock() 
        self.mock_resource_agent.generate_resource_report = self._mock_resource_analysis
        
        # Mock Orchestrator
        self.mock_orchestrator = Mock()
        self.mock_orchestrator.execute_full_analysis = self._mock_full_workflow
        
    async def _mock_risk_analysis(self, location: str, **kwargs) -> Dict[str, Any]:
        """Mock risk analysis response"""
        await asyncio.sleep(0.1)  # Simulate API delay
        return {
            "location": location,
            "risk_score": 73.2,
            "risk_level": "HIGH",
            "trends": {
                "cases": {"growth_rate": 0.15, "trend": "INCREASING"},
                "hospitals": {"icu_occupancy": 0.87, "strain_level": "HIGH"}
            },
            "confidence": 92.5,
            "timestamp": time.time()
        }
        
    async def _mock_guidance_generation(self, risk_data: Dict, **kwargs) -> Dict[str, Any]:
        """Mock guidance generation response"""
        await asyncio.sleep(0.1)
        return {
            "location": risk_data.get("location", "Unknown"),
            "risk_level": risk_data.get("risk_level", "MODERATE"),
            "guidance": {
                "general_public": {
                    "headline": "Health Alert: Increased Precautions Recommended",
                    "key_actions": ["Consider wearing masks", "Maintain good hygiene"],
                    "urgency": "MODERATE"
                },
                "healthcare_workers": {
                    "headline": "Clinical Alert: Enhanced Protocols",
                    "key_actions": ["Implement enhanced PPE", "Review surge capacity"],
                    "urgency": "HIGH"
                }
            },
            "confidence": 91.0,
            "timestamp": time.time()
        }
        
    async def _mock_resource_analysis(self, location: str, **kwargs) -> Dict[str, Any]:
        """Mock resource analysis response"""
        await asyncio.sleep(0.1)
        return {
            "location": location,
            "resource_needs": {
                "icu_beds": {"deficit_14d": 225, "recommendation": "IMMEDIATE_EXPANSION"},
                "ventilators": {"deficit_14d": 35, "recommendation": "PROCURE_ADDITIONAL"}
            },
            "procurement_recommendations": [
                {"item": "Portable ICU beds", "quantity": 150, "urgency": "IMMEDIATE"}
            ],
            "confidence": 88.5,
            "timestamp": time.time()
        }
        
    async def _mock_full_workflow(self, query: str, location: str = "California") -> Dict[str, Any]:
        """Mock full workflow orchestration"""
        logger.info(f"ğŸ”„ Executing mock workflow for: {query}")
        
        # Simulate multi-agent coordination
        risk_result = await self._mock_risk_analysis(location)
        guidance_result = await self._mock_guidance_generation(risk_result)
        resource_result = await self._mock_resource_analysis(location)
        
        return {
            "query": query,
            "location": location,
            "risk_analysis": risk_result,
            "public_guidance": guidance_result,
            "resource_report": resource_result,
            "workflow_status": "COMPLETED",
            "execution_time": 0.5,
            "confidence": 90.0,
            "timestamp": time.time()
        }
        
    async def test_individual_agents(self) -> bool:
        """Test each agent individually"""
        logger.info("ğŸ¤– Testing individual agents...")
        
        all_passed = True
        
        # Test Data Intelligence Agent
        try:
            result = await self.mock_data_agent.analyze_outbreak_risk("California")
            assert result["risk_score"] > 0
            assert result["risk_level"] in ["LOW", "MODERATE", "HIGH", "CRITICAL"]
            assert result["confidence"] > 80
            logger.info("âœ“ Data Intelligence Agent test passed")
            self._record_test_result("Data Intelligence Agent", True)
        except Exception as e:
            logger.error(f"âŒ Data Intelligence Agent test failed: {e}")
            self._record_test_result("Data Intelligence Agent", False, str(e))
            all_passed = False
            
        # Test Public Guidance Agent
        try:
            mock_risk = {"location": "California", "risk_level": "HIGH"}
            result = await self.mock_guidance_agent.generate_public_guidance(mock_risk)
            assert "guidance" in result
            assert "general_public" in result["guidance"]
            assert "healthcare_workers" in result["guidance"]
            logger.info("âœ“ Public Guidance Agent test passed")
            self._record_test_result("Public Guidance Agent", True)
        except Exception as e:
            logger.error(f"âŒ Public Guidance Agent test failed: {e}")
            self._record_test_result("Public Guidance Agent", False, str(e))
            all_passed = False
            
        # Test Resource Report Agent
        try:
            result = await self.mock_resource_agent.generate_resource_report("California")
            assert "resource_needs" in result
            assert "procurement_recommendations" in result
            assert result["confidence"] > 80
            logger.info("âœ“ Resource Report Agent test passed")
            self._record_test_result("Resource Report Agent", True)
        except Exception as e:
            logger.error(f"âŒ Resource Report Agent test failed: {e}")
            self._record_test_result("Resource Report Agent", False, str(e))
            all_passed = False
            
        return all_passed
        
    async def test_workflow_orchestration(self) -> bool:
        """Test multi-agent workflow orchestration"""
        logger.info("ğŸ”„ Testing workflow orchestration...")
        
        try:
            query = "Analyze COVID-19 surge in California with resource planning"
            result = await self.mock_orchestrator.execute_full_analysis(query)
            
            # Validate workflow completion
            assert result["workflow_status"] == "COMPLETED"
            assert "risk_analysis" in result
            assert "public_guidance" in result
            assert "resource_report" in result
            assert result["confidence"] > 85
            
            logger.info("âœ“ Workflow orchestration test passed")
            self._record_test_result("Workflow Orchestration", True)
            return True
            
        except Exception as e:
            logger.error(f"âŒ Workflow orchestration test failed: {e}")
            self._record_test_result("Workflow Orchestration", False, str(e))
            return False
            
    async def test_demo_chat_cases(self) -> bool:
        """Test all demo chat cases from the test suite"""
        logger.info("ğŸ’¬ Testing demo chat cases...")
        
        if not self.mock_data_loaded:
            logger.error("âŒ Test cases not loaded")
            return False
            
        passed_cases = 0
        total_cases = len(self.test_cases)
        
        for case in self.test_cases:
            try:
                case_id = case["id"]
                initial_query = case["initial_query"]
                expected_capabilities = case["expected_capabilities"]
                success_criteria = case["success_criteria"]
                
                logger.info(f"ğŸ“ Testing case: {case_id}")
                
                # Execute workflow for this case
                result = await self.mock_orchestrator.execute_full_analysis(
                    initial_query, 
                    location=case.get("test_data", {}).get("location", "California")
                )
                
                # Validate against success criteria
                case_passed = await self._validate_success_criteria(result, success_criteria)
                
                if case_passed:
                    logger.info(f"âœ“ Case {case_id} passed")
                    passed_cases += 1
                else:
                    logger.error(f"âŒ Case {case_id} failed")
                    
                self._record_test_result(f"Demo Case: {case_id}", case_passed)
                
            except Exception as e:
                logger.error(f"âŒ Case {case.get('id', 'unknown')} error: {e}")
                self._record_test_result(f"Demo Case: {case.get('id', 'unknown')}", False, str(e))
                
        success_rate = (passed_cases / total_cases) * 100
        logger.info(f"ğŸ“Š Demo cases success rate: {success_rate:.1f}% ({passed_cases}/{total_cases})")
        
        return success_rate >= 80.0  # Require 80% success rate
        
    async def _validate_success_criteria(self, result: Dict[str, Any], criteria: Dict[str, Any]) -> bool:
        """Validate result against success criteria"""
        try:
            # Check confidence threshold
            if "confidence_threshold" in criteria:
                if result.get("confidence", 0) < criteria["confidence_threshold"]:
                    return False
                    
            # Check specific requirements
            if criteria.get("risk_score_generated", False):
                if not result.get("risk_analysis", {}).get("risk_score"):
                    return False
                    
            if criteria.get("multi_audience_guidance", False):
                guidance = result.get("public_guidance", {}).get("guidance", {})
                if not ("general_public" in guidance and "healthcare_workers" in guidance):
                    return False
                    
            if criteria.get("resource_deficit_identified", False):
                resource_needs = result.get("resource_report", {}).get("resource_needs", {})
                if not resource_needs:
                    return False
                    
            return True
            
        except Exception:
            return False
            
    def _record_test_result(self, test_name: str, passed: bool, error: Optional[str] = None):
        """Record test result"""
        self.test_results.append({
            "test_name": test_name,
            "passed": passed,
            "error": error,
            "timestamp": time.time()
        })
        
    async def test_error_handling(self) -> bool:
        """Test error handling and resilience"""
        logger.info("âš ï¸ Testing error handling...")
        
        try:
            # Test with invalid location
            result = await self.mock_data_agent.analyze_outbreak_risk("")
            # Should handle gracefully
            
            # Test with malformed data
            result = await self.mock_guidance_agent.generate_public_guidance({})
            # Should provide fallback response
            
            logger.info("âœ“ Error handling test passed")
            self._record_test_result("Error Handling", True)
            return True
            
        except Exception as e:
            logger.error(f"âŒ Error handling test failed: {e}")
            self._record_test_result("Error Handling", False, str(e))
            return False
            
    async def test_performance_benchmarks(self) -> bool:
        """Test performance and response times"""
        logger.info("âš¡ Testing performance benchmarks...")
        
        try:
            start_time = time.time()
            
            # Execute multiple concurrent workflows
            tasks = []
            for i in range(5):
                task = self.mock_orchestrator.execute_full_analysis(f"Test query {i}")
                tasks.append(task)
                
            results = await asyncio.gather(*tasks)
            
            end_time = time.time()
            total_time = end_time - start_time
            avg_time = total_time / len(tasks)
            
            # Validate performance criteria
            if avg_time < 2.0:  # Average response under 2 seconds
                logger.info(f"âœ“ Performance test passed (avg: {avg_time:.2f}s)")
                self._record_test_result("Performance Benchmarks", True)
                return True
            else:
                logger.error(f"âŒ Performance test failed (avg: {avg_time:.2f}s)")
                self._record_test_result("Performance Benchmarks", False, f"Average time: {avg_time:.2f}s")
                return False
                
        except Exception as e:
            logger.error(f"âŒ Performance test failed: {e}")
            self._record_test_result("Performance Benchmarks", False, str(e))
            return False
            
    def generate_test_report(self) -> Dict[str, Any]:
        """Generate comprehensive test report"""
        passed_tests = sum(1 for result in self.test_results if result["passed"])
        total_tests = len(self.test_results)
        success_rate = (passed_tests / total_tests * 100) if total_tests > 0 else 0
        
        report = {
            "summary": {
                "total_tests": total_tests,
                "passed_tests": passed_tests,
                "failed_tests": total_tests - passed_tests,
                "success_rate": success_rate,
                "timestamp": time.time()
            },
            "test_results": self.test_results,
            "status": "PASSED" if success_rate >= 90 else "FAILED",
            "recommendations": self._generate_recommendations()
        }
        
        return report
        
    def _generate_recommendations(self) -> List[str]:
        """Generate recommendations based on test results"""
        recommendations = []
        
        failed_tests = [result for result in self.test_results if not result["passed"]]
        
        if failed_tests:
            recommendations.append("Review failed test cases and address underlying issues")
            
        if len([r for r in self.test_results if "Performance" in r["test_name"] and not r["passed"]]) > 0:
            recommendations.append("Optimize performance - consider caching and parallel processing")
            
        if len([r for r in self.test_results if "Error Handling" in r["test_name"] and not r["passed"]]) > 0:
            recommendations.append("Improve error handling and fallback mechanisms")
            
        if not recommendations:
            recommendations.append("All tests passed - system is ready for deployment")
            
        return recommendations

async def main():
    """Main test execution"""
    logger.info("ğŸš€ Starting Phoenix Outbreak Intelligence Integration Tests")
    logger.info("=" * 60)
    
    # Initialize test suite
    test_suite = IntegrationTestSuite()
    
    # Setup test environment
    setup_success = await test_suite.setup_test_environment()
    if not setup_success:
        logger.error("âŒ Failed to setup test environment")
        sys.exit(1)
        
    # Run test suite
    test_results = []
    
    try:
        # Test individual agents
        agents_passed = await test_suite.test_individual_agents()
        test_results.append(("Individual Agents", agents_passed))
        
        # Test workflow orchestration
        workflow_passed = await test_suite.test_workflow_orchestration()
        test_results.append(("Workflow Orchestration", workflow_passed))
        
        # Test demo chat cases
        demo_cases_passed = await test_suite.test_demo_chat_cases()
        test_results.append(("Demo Chat Cases", demo_cases_passed))
        
        # Test error handling
        error_handling_passed = await test_suite.test_error_handling()
        test_results.append(("Error Handling", error_handling_passed))
        
        # Test performance
        performance_passed = await test_suite.test_performance_benchmarks()
        test_results.append(("Performance", performance_passed))
        
    except Exception as e:
        logger.error(f"âŒ Test execution failed: {e}")
        sys.exit(1)
        
    # Generate and display report
    report = test_suite.generate_test_report()
    
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ“Š INTEGRATION TEST REPORT")
    logger.info("=" * 60)
    logger.info(f"Total Tests: {report['summary']['total_tests']}")
    logger.info(f"Passed: {report['summary']['passed_tests']}")
    logger.info(f"Failed: {report['summary']['failed_tests']}")
    logger.info(f"Success Rate: {report['summary']['success_rate']:.1f}%")
    logger.info(f"Status: {report['status']}")
    
    logger.info("\nğŸ“ Recommendations:")
    for recommendation in report["recommendations"]:
        logger.info(f"  â€¢ {recommendation}")
        
    # Save detailed report
    with open('tests/integration_test_report.json', 'w') as f:
        json.dump(report, f, indent=2)
    logger.info(f"\nğŸ“„ Detailed report saved to: tests/integration_test_report.json")
    
    # Exit with appropriate code
    if report["status"] == "PASSED":
        logger.info("\nğŸ‰ All integration tests passed! Phoenix is ready for production.")
        sys.exit(0)
    else:
        logger.error("\nâŒ Some integration tests failed. Review and fix before deployment.")
        sys.exit(1)

if __name__ == "__main__":
    asyncio.run(main())